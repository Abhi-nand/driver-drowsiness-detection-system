{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import scipy\n",
    "from keras.preprocessing import image\n",
    "import numpy as np\n",
    "import random\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dropout,Conv2D,Flatten,Dense,BatchNormalization\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will be using ImageDataGenerator.flow_from_directory() for generating the data of each sample in training set and storing it into train_batch for training purpose and into valid_batch for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1234 images belonging to 2 classes.\n",
      "Found 218 images belonging to 1 classes.\n",
      "39\n",
      "38 6\n"
     ]
    }
   ],
   "source": [
    "def generator(dir, gen=image.ImageDataGenerator(rescale=1./255), shuffle=True,batch_size=1,target_size=(24,24),\n",
    "              class_mode='categorical' ):\n",
    "    return gen.flow_from_directory(dir,batch_size=batch_size,shuffle=shuffle,color_mode='grayscale',\n",
    "                                   class_mode=class_mode,target_size=target_size)\n",
    "\"\"\"\n",
    "target_size - Tuple of integers (height, width), defaults to (256,256). The dimensions to which all images found will be resized.\n",
    "batch_size - Size of the batches of data (default: 32).\n",
    "SPE - steps per epochs - Number of train_batches passed to the model such that it train the whole dataset once \n",
    "                        (one epoch completed).\n",
    "VS - validation steps - Number of valid_batches passed to the model after finished training once ( after one epoch).\n",
    "\"\"\"\n",
    "BS= 32\n",
    "TS=(24,24)\n",
    "\n",
    "train_batch= generator('data/train',shuffle=True, batch_size=BS,target_size=TS)\n",
    "valid_batch= generator('data/valid',shuffle=True, batch_size=BS,target_size=TS)\n",
    "\n",
    "print(len(train_batch))\n",
    "SPE= len(train_batch.classes)//BS\n",
    "VS = len(valid_batch.classes)//BS\n",
    "print(SPE,VS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first layer in the model is a 2-dimensional convolutional layer. This layer will have 32 output filters each with a kernel size of 3x3, and we'll use the relu activation function.\n",
    "On the first layer only, we also specify the input_shape, which is the shape of our data.Here , we have input_shape of (24,24,1).We follow this by adding another convolutional layer with the exact specs as the earlier one, and added one more convolutional layer except for this third Conv2D layer has 64 filters. The choice of 64 here is again arbitrary, but the general choice of having more filters in later layers than in earlier ones is common.\n",
    "\n",
    "Dense layer is the output layer of the network, and so it has 2 nodes, one for open eyes and one for closed eyes. We'll use the softmax activation function on our output so that the output for each sample is a probability distribution over the outputs of\n",
    "open eye and  closed eye."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model has been created \n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    # 32 convolution filters used each of size 3x3\n",
    "    Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(24,24,1)),\n",
    "    \n",
    "    # 32 convolution filters used each of size 3x3\n",
    "    Conv2D(32,(3,3),activation='relu'),\n",
    "    \n",
    "    # 64 convolution filters used each of size 3x3\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    \n",
    "    # dropout - it is a regularization technique that randomly ignores a subset of nodes in a given layer\n",
    "    # to improve convergence and to reduce overfiting conditions. \n",
    "    # A model is said to be underfitting when it's not able to classify the data it was trained on. \n",
    "    Dropout(0.25),\n",
    "    \n",
    "    #  flattening -  output shape is (batch, 1).\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(2, activation='softmax')\n",
    "])\n",
    "print('model has been created ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Closed': 0, 'Open': 1}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_batch.class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "38/38 [==============================] - 106s 2s/step - loss: 0.0405 - accuracy: 0.9841 - val_loss: 0.0902 - val_accuracy: 0.9635\n",
      "Epoch 2/15\n",
      "38/38 [==============================] - 18s 467ms/step - loss: 0.0188 - accuracy: 0.9978 - val_loss: 0.0987 - val_accuracy: 0.9635\n",
      "Epoch 3/15\n",
      "38/38 [==============================] - 18s 481ms/step - loss: 0.0103 - accuracy: 0.9966 - val_loss: 0.0671 - val_accuracy: 0.9635\n",
      "Epoch 4/15\n",
      "38/38 [==============================] - 17s 446ms/step - loss: 0.0093 - accuracy: 0.9968 - val_loss: 0.0358 - val_accuracy: 0.9792\n",
      "Epoch 5/15\n",
      "38/38 [==============================] - 28s 755ms/step - loss: 0.0186 - accuracy: 0.9925 - val_loss: 0.1015 - val_accuracy: 0.9688\n",
      "Epoch 6/15\n",
      "38/38 [==============================] - 21s 552ms/step - loss: 0.0074 - accuracy: 0.9983 - val_loss: 0.0852 - val_accuracy: 0.9740\n",
      "Epoch 7/15\n",
      "38/38 [==============================] - 17s 444ms/step - loss: 0.0052 - accuracy: 0.9978 - val_loss: 0.1411 - val_accuracy: 0.9688\n",
      "Epoch 8/15\n",
      "38/38 [==============================] - 17s 450ms/step - loss: 0.0231 - accuracy: 0.9941 - val_loss: 0.1308 - val_accuracy: 0.9635\n",
      "Epoch 9/15\n",
      "38/38 [==============================] - 17s 452ms/step - loss: 0.0283 - accuracy: 0.9928 - val_loss: 0.1217 - val_accuracy: 0.9531\n",
      "Epoch 10/15\n",
      "38/38 [==============================] - 17s 456ms/step - loss: 0.0078 - accuracy: 0.9979 - val_loss: 0.1177 - val_accuracy: 0.9635\n",
      "Epoch 11/15\n",
      "38/38 [==============================] - 19s 485ms/step - loss: 0.0043 - accuracy: 0.9996 - val_loss: 0.1349 - val_accuracy: 0.9635\n",
      "Epoch 12/15\n",
      "38/38 [==============================] - 19s 494ms/step - loss: 0.0073 - accuracy: 0.9979 - val_loss: 0.1720 - val_accuracy: 0.9688\n",
      "Epoch 13/15\n",
      "38/38 [==============================] - 18s 479ms/step - loss: 0.0081 - accuracy: 0.9973 - val_loss: 0.0385 - val_accuracy: 0.9740\n",
      "Epoch 14/15\n",
      "38/38 [==============================] - 18s 470ms/step - loss: 0.0082 - accuracy: 0.9971 - val_loss: 0.1116 - val_accuracy: 0.9740\n",
      "Epoch 15/15\n",
      "38/38 [==============================] - 19s 492ms/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.1138 - val_accuracy: 0.9740\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0xf28839a880>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Now that the model is built, we compile and train the model using the Adam optimizer,a loss of categorical_cross_entropy\n",
    "# epochs - An epoch refers to a single pass of the entire dataset to the network during training.\n",
    "\n",
    "model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "model.fit(train_batch,validation_data=valid_batch,epochs=15,steps_per_epoch=SPE ,validation_steps=VS)\n",
    "# Avoid retraning the model for avoiding overfitting condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('models/cnn.h5', overwrite=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
